
## Inspiration
Inclusivity 
## What it does

## How we built it

## Challenges we ran into
* **Brainstorming and Research:** Identifying pain points for differently-abled users on websites and applications, required gathering insights from diverse user groups to address a wide range of accessibility needs
* **Choosing the Right Model:** Selecting the appropriate image-to-text model for our use case was difficult, as we had to consider factors like time complexity, CPU requirements, and cost.

*  **Crafting the Right Prompt:**
Fine Tuning the Model

* **Grammatically improvising speech:**


## Accomplishments that we're proud of
* We decided to focus on a single disability—visual impairment—and identified pain points for visually impaired users by testing with various screen readers
* We implemented models from scratch, tested various available open-source models, and chose the most apt ones:
    * Image-to-Text: GPT-2, Molmo, Hugging Face (Microsoft Tesseract OCR, Donut, BLIP, CLIP), Groq (Llama-3.2-11b-vision-preview, Llama-3.2-90b-vision-preview)
    * Text-to-Speech: pyttsx3, gTTS

## What we learned

## What's next for Untitled
